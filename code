# 1) Data Cleaning 

rm(list = ls(all.names = TRUE))
gc()
cat("\014")  

april_to_june_2019 <- read.csv("/Users/SakinaMNazarali/Downloads/2019_Yellow_Taxi_Trip_Data_April_to_June.csv")
april_to_june_2020 <- read.csv("/Users/SakinaMNazarali/Downloads/2020_Yellow_Taxi_Trip_Data_April_to_June.csv")
taxi_zones <- read.csv("/Users/SakinaMNazarali/Downloads/taxi+_zone_lookup.csv")
nrow(april_to_june_2019) # 21772272
nrow(april_to_june_2020) # 1110871 

set.seed(12345)
install.packages("dplyr")
install.packages("data.table")
library(dplyr)
require(data.table)
require(pastecs)

# The 2019 dataset has over 21 million records. Randomly select 2 million records.
atj_2019 <- sample_n(april_to_june_2019,2000000)
nrow(atj_2019) # 2000000
atj_2020 <- april_to_june_2020
str(atj_2019)
str(atj_2020)

# The dates are currently incharacters format + AM/PM. In order to be able to perform an analysis on
# them, they need to be changed to POSIXct + military timing
str(atj_2019$tpep_pickup_datetime) #  chr 
str(atj_2019$tpep_dropoff_datetime) #  chr 
str(atj_2020$tpep_pickup_datetime) #  chr 
str(atj_2020$tpep_dropoff_datetime) #  chr 
install.packages("lubridate")
install.packages("tidyverse")
library(lubridate)
atj_2019$tpep_pickup_datetime = parse_date_time(atj_2019$tpep_pickup_datetime, c('Ymd HM', 'mdy IMS p'))
str(atj_2019$tpep_pickup_datetime) # POSIXct
atj_2019$tpep_dropoff_datetime = parse_date_time(atj_2019$tpep_dropoff_datetime, c('Ymd HM', 'mdy IMS p'))
str(atj_2019$tpep_dropoff_datetime) # POSIXct
atj_2020$tpep_pickup_datetime = parse_date_time(atj_2020$tpep_pickup_datetime, c('Ymd HM', 'mdy IMS p'))
str(atj_2020$tpep_pickup_datetime) # POSIXct
atj_2020$tpep_dropoff_datetime = parse_date_time(atj_2020$tpep_dropoff_datetime, c('Ymd HM', 'mdy IMS p'))
str(atj_2020$tpep_dropoff_datetime) # POSIXct
# A time that was previously written as "06/18/2019 03:38:34 PM" is now written as "2019-06-18 15:38:34"

# Ensure that the data we have is within the timeframe we are interested in: 
nrow(atj_2019) # 2000000
atj_2019 <- atj_2019[atj_2019$tpep_pickup_datetime >= "2019-04-01 00:00:00" & atj_2019$tpep_pickup_datetime <= "2019-06-30 11:59:59",]
nrow(atj_2019) # 1998403
nrow(atj_2020) # 1110871
atj_2020 <- atj_2020[atj_2020$tpep_pickup_datetime >= "2020-04-01 00:00:00" & atj_2020$tpep_pickup_datetime <= "2020-06-30 11:59:59",]
nrow(atj_2020) # 1109997
# Therefore, data records that were beyond our timeframe have now been ommitted. Note the difference in number of
# rows before filtering for the times.


# Check if the randomization was fair (only performed on 2019 data, given that the dataset was huge) and that each month has an approximately 
# equal number of records, subset the data by month:
april_2019 = atj_2019[atj_2019$tpep_pickup_datetime >= "2019-04-01 00:00:00" & atj_2019$tpep_pickup_datetime <= "2019-04-30 11:59:59",]
may_2019 = atj_2019[atj_2019$tpep_pickup_datetime >= "2019-05-01 00:00:00" & atj_2019$tpep_pickup_datetime <= "2019-05-31 11:59:59",]
june_2019 = atj_2019[atj_2019$tpep_pickup_datetime >= "2019-06-01 00:00:00" & atj_2019$tpep_pickup_datetime <= "2019-06-30 11:59:59",]
nrow(april_2019) # 673954
nrow(may_2019) # 687271
nrow(june_2019) # 618196

# To perform a similar check on 2020 data (even though the dataset was not randomly sampled), subset the data by months:
april_2020 = atj_2020[atj_2020$tpep_pickup_datetime >= "2020-04-01 00:00:00" & atj_2020$tpep_pickup_datetime <= "2020-04-30 11:59:59",]
may_2020 = atj_2020[atj_2020$tpep_pickup_datetime >= "2020-05-01 00:00:00" & atj_2020$tpep_pickup_datetime <= "2020-05-31 11:59:59",]
june_2020 = atj_2020[atj_2020$tpep_pickup_datetime >= "2020-06-01 00:00:00" & atj_2020$tpep_pickup_datetime <= "2020-06-30 11:59:59",]
nrow(april_2020) # 235508
nrow(may_2020) # 345519
nrow(june_2020) # 522945

# Drop unnecessary columns
ncol(atj_2019) # 18
atj_2019 = select(atj_2019, -c(store_and_fwd_flag,improvement_surcharge,congestion_surcharge, mta_tax))
ncol(atj_2019) # 14
ncol(atj_2020) # 18 
atj_2020 = select(atj_2020, -c(store_and_fwd_flag,improvement_surcharge,congestion_surcharge, mta_tax))
ncol(atj_2020) # 14

# For individuals who made a cash payment, the tip fare is recorded as 0, even though they may have made a tip. 
# The total fare of the trip is present in the dataset, but this would mean that tip/total fare = 0 for such rides
# which may skew the data.

# The number of trips that were performed using cash payment in the 2019 and 2020 dataset: 
sum(atj_2019$payment_type=='2') # 544475
sum(atj_2019$payment_type=='2')/nrow(atj_2019) # 0.2724586 = 27.2% of all trips in 2019 were paid via cash
sum(atj_2020$payment_type=='2', na.rm = TRUE) # 353495
sum(atj_2020$payment_type=='2', na.rm = TRUE)/nrow(atj_2020) # 0.3184648 = 31.8% of all trips in 2020 were paid via cash

# Drop all rows that were performed using a cash payment. payment_type == 2. 
nrow(atj_2019) # 1998377
atj_2019 <- subset(atj_2019, payment_type!="2")
nrow(atj_2019) # 1453902
nrow(atj_2020) # 1109997
atj_2020 <- subset(atj_2020, payment_type!="2")
nrow(atj_2020) # 629492

# Merge the location file to associate pick up and drop off locations with names over numbers.
# Whilst this does not necessarily make a difference in the analysis, it will in the interpretation. 
merged2019 <- merge(atj_2019,taxi_zones,by.x = c("PULocationID"),by.y = c("LocationID"))
names(merged2019)[names(merged2019) == 'Zone'] <- 'PUZone'
names(merged2019)[names(merged2019) == 'Borough'] <- 'PUBorough'
names(merged2019)[names(merged2019) == 'service_zone'] <- 'PUservice_zone'
merged2019 <- merge(merged2019,taxi_zones,by.x = c("DOLocationID"),by.y = c("LocationID"))
names(merged2019)[names(merged2019) == 'Zone'] <- 'DOZone'
names(merged2019)[names(merged2019) == 'Borough'] <- 'DOBorough'
names(merged2019)[names(merged2019) == 'service_zone'] <- 'DOservice_zone'
colnames(merged2019)

# [1] "DOLocationID"          "PULocationID"          "VendorID"              "tpep_pickup_datetime" 
# [5] "tpep_dropoff_datetime" "passenger_count"       "trip_distance"         "RatecodeID"           
# [9] "payment_type"          "fare_amount"           "extra"                 "tip_amount"           
# [13] "tolls_amount"          "total_amount"          "PUBorough"             "PUZone"               
#[17] "PUservice_zone"        "DOBorough"             "DOZone"                "DOservice_zone"       
# [21] "tipdivtotal"           "trip_time"             "numeric_trip_time"     "binary" 

merged2020 <- merge(atj_2020,taxi_zones,by.x = c("PULocationID"),by.y = c("LocationID"))
names(merged2020)[names(merged2020) == 'Zone'] <- 'PUZone'
names(merged2020)[names(merged2020) == 'Borough'] <- 'PUBorough'
names(merged2020)[names(merged2020) == 'service_zone'] <- 'PUservice_zone'
merged2020 <- merge(merged2020,taxi_zones,by.x = c("DOLocationID"),by.y = c("LocationID"))
names(merged2020)[names(merged2020) == 'Zone'] <- 'DOZone'
names(merged2020)[names(merged2020) == 'Borough'] <- 'DOBorough'
names(merged2020)[names(merged2020) == 'service_zone'] <- 'DOservice_zone'
colnames(merged2020)

# Checking if any columns have NA Values
names(merged2019)[sapply(merged2019, anyNA)] # "PUZone" "DOZone"
names(merged2020)[sapply(merged2020, anyNA)] # "PUZone" "DOZone"
# As mentioned earlier, the zone information (that were merged later on) are not of use in the analysis,
# but will come handy in the interpretation. Therefore, these data points will not need to be replaced or omitted.
# NAs are associated with Pick up and Drop Off locations 264 and 265. 

# Add a column which calculates the tip per trip as a fraction of the total fare
merged2019$tipdivtotal <- merged2019$tip_amount/merged2019$total_amount
merged2020$tipdivtotal <- merged2020$tip_amount/merged2020$total_amount
head(merged2019)
head(merged2020)

predata2019 <- merged2019
predata2020 <- merged2020

library(ggplot2)
ggplot(merged2019, na.rm = TRUE, aes(x=tip_amount,y=tipdivtotal)) + geom_point() + geom_smooth(method=lm, se=FALSE, col='blue', size=1) + ggtitle("Tip/Total vs Total Tip Amount 2019 Data Points")
ggplot(merged2020, na.rm = TRUE, aes(x=tip_amount,y=tipdivtotal)) + geom_point() + geom_smooth(method=lm, se=FALSE, col='blue', size=1) + ggtitle("Tip/Total vs Total Tip Amount 2020 Data Points")

# Get rid of rows in the dataframe that have a negative tipdivtotal (You cannot have a negative tip)
min(merged2019$tipdivtotal) # -1.0142
dim(merged2019) # 1453348 
merged2019 <- subset(merged2019,tipdivtotal >= 0)
min(merged2019$tipdivtotal) # 0 
dim(merged2019) # 1453114 

min(merged2020$tipdivtotal, na.rm=TRUE) # -49.44776
dim(merged2020) # 629492 
merged2020 <- subset(merged2020,tipdivtotal >= 0)
min(merged2019$tipdivtotal) # 0 
dim(merged2020) # 629128 

# Get rid of rows in the dataframe that have negative tip_amount
min(merged2019$tip_amount) # -88.88
dim(merged2019)
merged2019 <- subset(merged2019, tip_amount >= 0)
min(merged2019$tip_amount) # 0 
dim(merged2019)

min(merged2020$tip_amount) # -36.3
dim(merged2020)
merged2020 <- subset(merged2020, tip_amount >= 0)
min(merged2020$tip_amount) # 0 
dim(merged2020)

# Look at the tip_amount table and observe anomalies
summary(merged2019$tip_amount)
# Min. 1st Qu.  Median    Mean    3rd Qu.    Max. 
# 0.000   1.700   2.360   3.087   3.460 333.330 
summary(merged2020$tip_amount)
# Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
# 0.000   1.460   2.160   2.583   3.060 442.180 

# Looking at the maximum tip amounts, these could be heavily skewing the data. 
# Eliminate all rows with tips more than $50 
merged2019 <- subset(merged2019, tip_amount <= 50)
merged2020 <- subset(merged2020, tip_amount <= 50)

# Space total_amount between 0 and 200 # Max is 9435 hence outlier
merged2019 <- subset(merged2019, total_amount <= 200)
merged2020 <- subset(merged2020, total_amount <= 200)
merged2019 <- subset(merged2019, total_amount >= 0)
merged2020 <- subset(merged2020, total_amount >= 0)

merged2019$tipdivtotal <- merged2019$tip_amount/merged2019$total_amount
merged2020$tipdivtotal <- merged2020$tip_amount/merged2020$total_amount

# Plot the data to see how they look after cleaning - big difference!
ggplot(merged2019, na.rm = TRUE, aes(x=tip_amount,y=tipdivtotal)) + geom_point() + geom_smooth(method=lm, se=FALSE, col='blue', size=1) + ggtitle("Tip/Total vs Total Tip Amount 2019 Data Points")
ggplot(merged2020, na.rm = TRUE, aes(x=tip_amount,y=tipdivtotal)) + geom_point() + geom_smooth(method=lm, se=FALSE, col='blue', size=1) + ggtitle("Tip/Total vs Total Tip Amount 2020 Data Points")

options(scipen=5)

# Check if tipdivtotal and tip_amount are normally distributed 
hist(merged2019$tipdivtotal)
hist(merged2020$tipdivtotal)

hist(merged2019$tipdivtotal,
     main="Tip as a Percentage of Total Fare Amounts 2019",
     xlab="TipDivTotal",
     col="darkmagenta",
     freq=FALSE)

hist(merged2020$tipdivtotal,
     main="Tip as a Percentage of Total Fare Amounts 2020",
     xlab="TipDivTotal",
     col="blue",
     freq=FALSE)

# Add a column that calculates the trip time (DOtime - PUtime)
merged2019$trip_time <- merged2019$tpep_dropoff_datetime - merged2019$tpep_pickup_datetime
head(merged2019$trip_time)
merged2020$trip_time <- merged2020$tpep_dropoff_datetime - merged2020$tpep_pickup_datetime
head(merged2020$trip_time)
str(merged2019$trip_time)

# Create a new column that holds the trip_time data as a numeric value, instead of a difftime value
merged2019$numeric_trip_time <- as.numeric(merged2019$trip_time)
str(merged2019$trip_time) # difftime
str(merged2019$numeric_trip_time) # num 
merged2020$numeric_trip_time <- as.numeric(merged2020$trip_time)
str(merged2020$numeric_trip_time) # num

# Get rid of rows in the dataframe that have a negative trip_time (You cannot have a negative travel time)
dim(merged2019) # 1453114 
min(merged2019$numeric_trip_time) # -344247
merged2019 <- subset(merged2019, numeric_trip_time >= 0)
min(merged2019$numeric_trip_time) # 0 
dim(merged2019) # 1453113 

dim(merged2020) # 629128
min(merged2020$numeric_trip_time) # -31873874
merged2020 <- subset(merged2020, numeric_trip_time >= 0)
min(merged2019$numeric_trip_time) # 0 
dim(merged2020) # 629127 

# Add a binary column that classifies the tip/totalamount as a binary variable based on the median tip amount
merged2019$binary <- ifelse(merged2019$tipdivtotal>median(merged2019$tipdivtotal,na.rm = TRUE),1,0)
merged2020$binary <- ifelse(merged2020$tipdivtotal>median(merged2020$tipdivtotal, na.rm = TRUE),1,0)
median(merged2020$tipdivtotal) # 0.166348

# Add a dummy variable that classifies thepickup time by the time of the day where if the pickup time is 
# between 00:00:00 and 05:00:00 = 1 Early Morning
# between 05:00:01 and 09:00:00 = 2 Morning Rush Hour
# between 09:00:01 and 16:00:00 = 3 Working Hours 
# between 16:00:01 and 20:00:00 = 4 Evening Rush Hour
# between 20:00:01 and 23:59:59 = 5 Night 
merged2019$PUdate <- as.Date(merged2019$tpep_pickup_datetime)
class(merged2019$PUdate)
merged2019$PUtime <- format(merged2019$tpep_pickup_datetime,format = "%H:%M:%S")
merged2019$PUtime <- as.times(merged2019$PUtime)
class(merged2019$PUtime)
library(chron)
breaks <- c('00:00:00', '05:00:00', '09:00:00', '16:00:00','20:00:00', '23:59:59')
labels <- c(1,2,3,4,5)
h1 <- chron(times=merged2019$PUtime)
br <- chron(times=breaks)
merged2019$time_of_day <-  cut(h1, br, labels=labels)
str(merged2019$time_of_day) # Factor
merged2019$time_of_day <- as.integer(merged2019$time_of_day)
head(merged2019)

merged2020$PUdate <- as.Date(merged2020$tpep_pickup_datetime)
class(merged2020$PUdate)
merged2020$PUtime <- format(merged2020$tpep_pickup_datetime,format = "%H:%M:%S")
merged2020$PUtime <- as.times(merged2020$PUtime)
class(merged2020$PUtime)
library(chron)
breaks <- c('00:00:00', '05:00:00', '09:00:00', '16:00:00','20:00:00', '23:59:59')
labels <- c(1,2,3,4,5)
h2 <- chron(times=merged2020$PUtime)
br <- chron(times=breaks)
merged2020$time_of_day <-  cut(h2, br, labels=labels)
str(merged2020$time_of_day) # Factor
merged2020$time_of_day <- as.integer(merged2020$time_of_day)
head(merged2020)

# Classify the days of the week using categorical/dummy variables therefore, to assign every date with a weekday.
# Weekdays are classified between 1-7 as shown below:
# Sunday = 1
# Monday = 2
# Saturday = 7
merged2019$dow <- as.POSIXlt(merged2019$PUdate)$wday + 1
str(merged2019$dow)
head(merged2019)
merged2020$dow <- as.POSIXlt(merged2020$PUdate)$wday + 1
str(merged2020$dow)
head(merged2020)

# Get rid of trip_distance field beyond 100 # Max is 830.90, hence outliers
merged2019 <- subset(merged2019, trip_distance <= 100)
merged2020 <- subset(merged2020, trip_distance <= 100)

# Final check 
names(merged2019) # check titles for each of the column
head(merged2019,5) # check the first five rows for each column
str(merged2019) # check for the data types of each column (integer, num)
class(merged2019) # check for the data class of the dataframe
summary(merged2019) # perform quick overview of data
dim(merged2019) # check for the number of columns by rows

names(merged2020)
head(merged2020,5)
str(merged2020)
class(merged2020)
summary(merged2020)
dim(merged2020)

# 2) Summary Analytics

# Summary statistics for the merged2019 dataset
library(stargazer)
stargazer(merged2019,type="text",summary.stat = c("min", "p25", "median","mean", "p75", "max","sd"))

# Summary statistics for the merged2020 dataset
stargazer(merged2020,type="text",summary.stat = c("min", "p25", "median","mean", "p75", "max","sd"))

# Correlation Matrix visualizations
install.packages("corrplot")
library(corrplot)
corrMatrix2019 <- cor(select_if(merged2019,is.numeric), use="complete.obs")
corrplot(corrMatrix2019, method="color",tl.col="black",tl.cex = 0.5, col=colorRampPalette(c("black","white","gold"))(100))
corrMatrix2020 <- cor(select_if(merged2020,is.numeric), use="complete.obs")
corrplot(corrMatrix2020,method="color",tl.col="black",tl.cex = 0.5, col=colorRampPalette(c("black","white","gold"))(100))

# Create an interaction variable between total_amount and trip_time
merged2019$totamt_x_tripdist <- merged2019$total_amount * merged2019$trip_distance
merged2020$totamt_x_tripdist <- merged2020$total_amount * merged2020$trip_distance

# Dividing the datasets based on high and low tip amounts
lowtips_2019 <- subset(merged2019, binary==0)
hightips_2019 <- subset(merged2019, binary==1)

# Summary analysis for 2019 dataset, for tips lower than the median
library(stargazer)
stargazer(lowtips_2019,type="text",summary.stat = c("min", "p25", "median","mean", "p75", "max","sd"))

# Summary analysis for 2019 dataset, for tips higher than the median
stargazer(hightips_2019,type="text",summary.stat = c("min", "p25", "median","mean", "p75", "max","sd"))

# Dividing the datasets based on high and low tip amounts
lowtips_2020 <- subset(merged2020, binary==0)
hightips_2020 <- subset(merged2020, binary==1)

# Summary analysis for 2019 dataset, for tips lower than the median
summary(lowtips_2020)

# Summary analysis for 2019 dataset, for tips higher than the median
summary(hightips_2020)

# Correlation Matrix 2019
cor(merged2019[, unlist(lapply(merged2019, is.numeric))], use="complete.obs") 

# Correlation Matrix 2002
cor(merged2020[, unlist(lapply(merged2020, is.numeric))], use="complete.obs") 

# Histograms for 2019 Data
par(mfrow=c(2,2))
hist(hightips_2019$trip_distance, main = "Trip Distance for High Tip 2019 Trips", col = "darkmagenta", freq = FALSE, xlim=c(0,50), xlab = "Trip Distance in Miles")
hist(lowtips_2019$trip_distance, main = "Trip Distance for Low Tip 2019 Trips", col = "aquamarine", freq = FALSE, xlab = "Trip Distance in Miles")
hist(hightips_2019$passenger_count, main = "Passenger Count for High Tip 2019 Trips", freq = FALSE, col = "burlywood4", xlab = "Passenger Counts")
hist(lowtips_2019$passenger_count, main = "Passenger Count for Low Tip 2019 Trips", freq = FALSE, col = "coral2", xlab = "Passenger Counts")
hist(hightips_2019$VendorID, main = "VendorID for High Tip 2019 Trips", freq = FALSE, col = "blueviolet", xlab = "VendorID")
hist(lowtips_2019$VendorID, main = "VendorID for Low Tip 2019 Trips", freq = FALSE, col = "lightblue1", xlab = "VendorID")
hist(hightips_2019$fare_amount, main = "Fare Amount for High Tip 2019 Trips", freq = FALSE, col =  "mediumorchid1", xlab =  "Fare Amount in $")
hist(lowtips_2019$fare_amount, main = "Fare Amount for Low Tip 2019 Trips", freq = FALSE, col =  "navajowhite", xlab =  "Fare Amount in $")
hist(hightips_2019$tipdivtotal, main = "Tip/Total for High 2019 Trips", freq = FALSE, col = "palevioletred1", xlab = "Fraction", xlim=c(0,0.3))
hist(lowtips_2019$tipdivtotal, main = "Tip/Total for Low 2019 Trips", freq = FALSE, col = "seagreen2", xlab = "Fraction", xlim=c(0,0.2))
hist(hightips_2019$numeric_trip_time, main ="Trip Time in Seconds for High 2019 Trips", freq = FALSE, col = "tan3", xlab = "Time in Seconds", xlim = c(0,5000))
hist(lowtips_2019$numeric_trip_time, main ="Trip Time in Seconds for Low 2019 Trips", freq = FALSE, col = "yellow", xlab = "Time in Seconds", xlim = c(0,5000))

# Histograms for 2020 Data
hist(hightips_2020$trip_distance, main = "Trip Distance for High Tip 2020 Trips", col = "darkmagenta", freq = FALSE, xlim=c(0,50), xlab = "Trip Distance in Miles")
hist(lowtips_2020$trip_distance, main = "Trip Distance for Low Tip 2020 Trips", col = "aquamarine", freq = FALSE, xlim=c(0,50), xlab = "Trip Distance in Miles")
hist(hightips_2020$passenger_count, main = "Passenger Count for High Tip 2020 Trips", freq = FALSE, col = "burlywood4", xlab = "Passenger Counts")
hist(lowtips_2020$passenger_count, main = "Passenger Count for Low Tip 2020 Trips", freq = FALSE, col = "coral2", xlab = "Passenger Counts")
hist(hightips_2020$VendorID, main = "VendorID for High Tip 2020 Trips", freq = FALSE, col = "blueviolet", xlab = "VendorID")
hist(lowtips_2020$VendorID, main = "VendorID for Low Tip 2020 Trips", freq = FALSE, col = "lightblue1", xlab = "VendorID")
hist(hightips_2020$fare_amount, main = "Fare Amount for High Tip 2020 Trips", freq = FALSE, col =  "mediumorchid1", xlab =  "Fare Amount in $")
hist(lowtips_2020$fare_amount, main = "Fare Amount for Low Tip 2020 Trips", freq = FALSE, col =  "navajowhite", xlab =  "Fare Amount in $")
hist(hightips_2020$tipdivtotal, main = "Tip/Total for High 2020 Trips", freq = FALSE, col = "palevioletred1", xlab = "Fraction", xlim=c(0,0.4))
hist(lowtips_2020$tipdivtotal, main = "Tip/Total for Low 2020 Trips", freq = FALSE, col = "seagreen2", xlab = "Fraction", xlim=c(0,0.4))
hist(hightips_2020$numeric_trip_time, main ="Trip Time in Seconds for High 2020 Trips", freq = FALSE, col = "tan3", xlab = "Time in Seconds", xlim = c(0,10000))
hist(lowtips_2020$numeric_trip_time, main ="Trip Time in Seconds for Low 2020 Trips", freq = FALSE, col = "yellow", xlab = "Time in Seconds", xlim = c(0,10000))

head(hightips_2019)
par(mfrow=c(1,1))


# 2) Question 1 - OLS - # What variables impact tipping behavior

# AIC of using tip_amount as DV - justify why (AIC score when compared to AIC of using tip_amount of DV, controls for outliers and extremes)

model1 <- lm(tipdivtotal ~ DOLocationID + VendorID + passenger_count + trip_distance + RatecodeID + total_amount + numeric_trip_time + time_of_day + dow + totamt_x_tripdist, data = merged2019)
summary(model1) # R2 = 0.05115 
model2 <- lm(tipdivtotal ~ VendorID + passenger_count + trip_distance + total_amount + numeric_trip_time + time_of_day + dow + totamt_x_tripdist, data = merged2019)
summary(model2) # R2 = 0.038
model3 <- lm(tipdivtotal ~ VendorID + passenger_count + trip_distance + total_amount + time_of_day + dow + totamt_x_tripdist, data = merged2019)
summary(model3) # R2 = 0.038
model4 <- lm(tipdivtotal ~ VendorID + passenger_count + trip_distance + total_amount + totamt_x_tripdist, data = merged2019)
summary(model4) # R2 = 0.03714
model5 <- lm(tip_amount ~ DOLocationID + VendorID + passenger_count + trip_distance + RatecodeID + total_amount + numeric_trip_time + time_of_day + dow + totamt_x_tripdist, data = merged2019)
summary(model5) # R2 = 0.7239

AIC(model1) # -4464100
AIC(model2) # -4445244
AIC(model3) # -4443607
AIC(model4) # -4442920
AIC(model5) # 5111731

require(data.table)
require(pastecs)
install.packages("stargazer")
library(stargazer)
stargazer(model1, model2, model3, model4, type = "text")

# Looking at AIC scores and the adjusted R2, model 1 is the best way to go. All the variables from model 1 are significant. 
# The reason why we have selected tipdivtotal as our DV is because it controls for anomalies. Sure, the R2 is great, but, 
# it is very prone to being skewed as result of huge tips on smaller trips. This could involve factors we cannot capture, like 
# the conversation that a passenger had with the driver or their relation. Therefore, tipdivtotal looked promising on controlling for these.
# Also note the AIC values. All models with a negative AIC are far more negative than the one where tip_amount is a DV.

# 3) Use LOGIT to refine the variables. Also use a LOGIT model to assess if it is possible to predict on 2019 data using 2019 data. This would include
# creating a train data and a test dataset. We would use our optimal LOGIT model to train the trained dataset (80%) and use it to predict on 20% of the 
# dataset (test)

# As one might expect, logistic regression makes ample use of the logistic function as it outputs values 
# between 0 and 1 which we can use to model and predict responses.

logitA <- glm(tipdivtotal ~ DOLocationID + VendorID + passenger_count + trip_distance + RatecodeID + total_amount + numeric_trip_time + time_of_day + dow + totamt_x_tripdist, data = merged2019, family = "binomial")
summary(logitA)
logitB <- glm(tipdivtotal ~ VendorID + passenger_count + trip_distance + total_amount + numeric_trip_time + time_of_day + dow + totamt_x_tripdist, data = merged2019, family = "binomial")
summary(logitB) #
logitC <- glm(tipdivtotal ~ VendorID + passenger_count + trip_distance + total_amount + time_of_day + dow + totamt_x_tripdist, data = merged2019, family = "binomial")
summary(logitC) 
logitD <- glm(tipdivtotal ~ VendorID + passenger_count + trip_distance + total_amount + totamt_x_tripdist, data = merged2019, family = "binomial")
summary(logitD) 

stargazer(logitA, logitB, logitC, logitD, type = "text")

# Interesting development. Here, logitD which represents the variables from model4 in OLS seems to have the lowest AIC. Additionally, time_of_day and dow have
# also lost some of their significance. Certainly however, is that VendorID, trip_distance, total_amount and the interaction variable, continue to show sustained significance. 

# Trying to refine our model using R's stepwise regression model (given that the results from LOGIT and OLS are not very consistent. Give the model all of the 
# variables it needs to makes its decision. 

install.packages("MASS")
library(MASS)
head(train2019)
KSmodel <- glm(binary ~ DOLocationID + VendorID + passenger_count + trip_distance + total_amount + 
                 numeric_trip_time + time_of_day + dow + totamt_x_tripdist, data = merged2019, family = "binomial")
SWReg <- stepAIC(KSmodel)   

# binary ~ VendorID + passenger_count + trip_distance + total_amount +  numeric_trip_time + time_of_day + dow + totamt_x_tripdist

# Df Deviance     AIC
# <none>                  1657760 1657778
# - dow                1  1657819 1657835
# - time_of_day        1  1657819 1657835
# - passenger_count    1  1657833 1657849
# - numeric_trip_time  1  1658703 1658719
# - totamt_x_tripdist  1  1659512 1659528
# - total_amount       1  1679874 1679890
# - trip_distance      1  1680591 1680607
# - VendorID           1  1968856 1968872

# Result suggests to use all the variables we fed the model apart from DOLocationID (it eliminates and adds back variables one by one 
# to find the model with the lowest AIC). This makes sense because when comparing the mean and medians of DOLocationIDs and their stdev, there 
# was not different (both were at 162). The model suggested by stepwise regression is a good balance of the models and gives us a good indication of the 
# model we should continue to use. 

# Use the optimal variables

merged2019$binary.factor <- factor(merged2019$binary)
m2019 <- sort(sample(nrow(merged2019), nrow(merged2019)*0.8))
train2019 <- merged2019[m2019,]
test2019 <- merged2019[-m2019,]
dim(train2019) # 1159940      30
dim(test2019) # 289985     30

train2019$time_of_day <- as.factor(train2019$time_of_day)
train2019$dow <- as.factor(train2019$dow)
train2019$VendorID <- as.factor(train2019$VendorID)
logitM <- glm(tipdivtotal ~ VendorID + passenger_count + trip_distance + total_amount + numeric_trip_time + time_of_day + dow + totamt_x_tripdist, data = train2019, family = "binomial")
summary(logitM)

# The results of this logit regression show that a certain vendorID(2) has a correlation with higher tips. 
# It also shows that time_of_day5 has a positive significant impact on tip variability. 
# Beyond these, the remaining variables are known to have an impact (even from OLS) with the exception of 
# passenger_count which is a control. 

# With our fitted model, we are interested in seeing how it would perform against our testing dataset. We can do 
# so by building a confusion matrix to display the success rate of the model's predictions on the testing dataset. 
# The predict function performs a prediction on a trip's tip based on the variables within the testing dataset 
# (total distance, trip time, tip amount, etc). The output of this function will provide us with probabilities. 
# The next command creates a vector of the 'Low' (low category, denoted as 0 in the dataset) with respect to the number
# of observations in the training data set. This is then converted into 'High' if the predicted probability is greater than half. 

# The table function builds the confusion matrix. 

test2019$time_of_day <- as.factor(test2019$time_of_day)
test2019$dow <- as.factor(test2019$dow)
test2019$VendorID <- as.factor(test2019$VendorID)

tips.prob = predict(logitM, test2019, type="response") # Predict on the test2019 test using the logitM model
tips.pred = rep("0",dim(train2019)[1]) # Place a "0" (low) on all of the rows in the tips.pred
tips.pred[tips.prob>.5] = "1" # if the predicted probability is greater than 0.5, then label it as "1" (high)
cmLOGIT <- table(tips.pred,train2019$binary.factor)
cmLOGIT

# tips.pred      0      1
#         0 579675 579752
#         1      9      3
# Where there is a match between 0,0 and 1,1, these are known as true positives and true positives, hence, 
# correctly predicted. 

# Calculate precision, accuracy, and recall 

# Assume that the model predicting a low tip when the tip is actually low is TP = 578505
# Assume that the model predicting a high tip when the tip is actually high is TN = 5
# Assume that the model predicting a high tip when the tip is actually low is FP = 3
# Assume that the model predicting a low tip when the tip is actually high is FN = 581427

# Calculate manually since computer is unable to get 'caret' package
cmLOGIT[1] # 578505
cmLOGIT[2] # 9
cmLOGIT[3] # 581427
cmLOGIT[4] # 3

accuracyLOGIT <- sum(cmLOGIT[1], cmLOGIT[4]) / sum(cmLOGIT[1:4]) 
accuracyLOGIT #  0.4999642
# The accuracy rate is really close to 50% which means that it is as efficient or slightly less than random guessing. 
# This could imply that the model being used is not a very strong one. How many did it correctly predict as true?

precisionLOGIT <- cmLOGIT[4] / sum(cmLOGIT[4], cmLOGIT[2])
precisionLOGIT # 0.25
# precision refers to how precise/accurate the model is when it comes to predicting positively. The model does not perform very well here. 

sensitivityLOGIT <- cmLOGIT[4] / sum(cmLOGIT[4], cmLOGIT[3])
sensitivityLOGIT # 0.0000051746
# Sensitivity or recall measures how well the model predicted the positives that it predicted properly in comparison to the ones that 
# were actually high tips. Here, the model has performed terribly, almost missing every high tip and predicting it as low. The model
# is very good at predicting low tips, but that is because it predicts pretty much everything as low tips. 

fscoreLOGIT <- (2 * (sensitivityLOGIT * precisionLOGIT))/(sensitivityLOGIT + precisionLOGIT)
fscoreLOGIT # 0.00001034899

# The F1 score indicates that the model does not do too well. The closer to 1 the score, the better. It measures for accuracy/
# reliability 

# 3) Use CART 

install.packages("rpart")
install.packages("rpart.plot")
library(rpart)
library(rpart.plot)

# Use the data created above for trained 2019 data to test on 2019 test data using CART

train2019 <- merged2019[m2019,]
test2019 <- merged2019[-m2019,]
prop.table(table(train2019$binary)) # Represent the column figures as percentages of the total 
prop.table(table(test2019$binary)) 

# CART prediction by inputting the variables 
train2019$time_of_day <- as.integer(train2019$time_of_day)
train2019$dow <- as.integer(train2019$dow)
train2019$VendorID <- as.integer(train2019$VendorID)
test2019$time_of_day <- as.integer(test2019$time_of_day)
test2019$dow <- as.integer(test2019$dow)
test2019$VendorID <- as.integer(test2019$VendorID)
CART1 <- rpart(binary ~ VendorID + passenger_count + trip_distance + total_amount + 
                 numeric_trip_time + time_of_day + dow + totamt_x_tripdist, data = train2019, method = 'class') 
C1 <- rpart.plot(CART1, fallen.leaves = TRUE, extra = 106) # tree plotting
C1 # Only divides the model from VendorID 
predict_CART1 <-predict(CART1, test2019, type = 'class')
confmatrixCART1 <- table(test2019$binary, predict_CART1)
confmatrixCART1 
CART1_accuracy <- sum(diag(confmatrixCART1)) / sum(confmatrixCART1)
CART1_accuracy # 0.7241427
CART1_precision <- confmatrixCART1[4]/sum(confmatrixCART1[4],confmatrixCART1[2])
CART1_precision # 0.8492959
CART1_sensitivity <- confmatrixCART1 [4] / sum(confmatrixCART1 [4], confmatrixCART1[3])
CART1_sensitivity # 0.6784105
CART1_fscore <- (2 * (CART1_sensitivity * CART1_precision))/(CART1_sensitivity + CART1_precision)
CART1_fscore # 0.7542958

# Train the dataset, again, on a different set of 80/20 data to check if the same split will occur. This is basically
# K Fold Regression, done manually. 
z2019 <- sort(sample(nrow(merged2019), nrow(merged2019)*0.8))
ztrain2019 <- merged2019[z2019,]
ztest2019 <- merged2019[-z2019,]
prop.table(table(ztrain2019$binary)) # Represent the column figures as percentages of the total 
prop.table(table(ztest2019$binary)) 
ztrain2019$time_of_day <- as.integer(ztrain2019$time_of_day)
ztrain2019$dow <- as.integer(ztrain2019$dow)
ztrain2019$VendorID <- as.integer(ztrain2019$VendorID)
ztest2019$time_of_day <- as.integer(ztest2019$time_of_day)
ztest2019$dow <- as.integer(ztest2019$dow)
ztest2019$VendorID <- as.integer(ztest2019$VendorID)
zCART1 <- rpart(binary ~ VendorID + passenger_count + trip_distance + total_amount + 
                   numeric_trip_time + time_of_day + dow + totamt_x_tripdist, data = ztrain2019, method = 'class') 
zC1 <- rpart.plot(zCART1, fallen.leaves = TRUE, extra = 106) # tree plotting
zC1 # Divides model from vendorID and then total_amount. Remember this is done on a newly sampled set of trained data. 
predict_zCART1 <-predict(zCART1, ztest2019, type = 'class')
confmatrix_zCART1 <- table(ztest2019$binary, predict_zCART1)
confmatrix_zCART1
zCART1_accuracy <- sum(diag(confmatrix_zCART1)) / sum(confmatrix_zCART1)
zCART1_accuracy # 0.7294004
zCART1_precision <- confmatrix_zCART1[4]/sum(confmatrix_zCART1[4],confmatrix_zCART1[2])
zCART1_precision # 0.8239527
zCART1_sensitivity <- confmatrix_zCART1[4] / sum(confmatrix_zCART1[4], confmatrix_zCART1[3])
zCART1_sensitivity # 0.6926399
zCART1_fscore <- (2 * (zCART1_sensitivity * zCART1_precision))/(zCART1_sensitivity + zCART1_precision)
zCART1_fscore # 0.7526115


# To see if a split through VendorID makes sense, test the split using a 70/30 data split. 
m2019_70 <- sort(sample(nrow(merged2019), nrow(merged2019)*0.7))
train2019_2 <- merged2019[m2019_70,]
test2019_2 <- merged2019[-m2019_70,]
prop.table(table(train2019_2$binary)) # Represent the column figures as percentages of the total 
prop.table(table(test2019_2$binary)) 
train2019_2$time_of_day <- as.integer(train2019_2$time_of_day)
train2019_2$dow <- as.integer(train2019_2$dow)
train2019_2$VendorID <- as.integer(train2019_2$VendorID)
test2019_2$time_of_day <- as.integer(test2019_2$time_of_day)
test2019_2$dow <- as.integer(test2019_2$dow)
test2019_2$VendorID <- as.integer(test2019_2$VendorID)
CART1_2 <- rpart(binary ~ VendorID + passenger_count + trip_distance + total_amount + 
                   numeric_trip_time + time_of_day + dow + totamt_x_tripdist, data = train2019_2, method = 'class') 
C1_2 <- rpart.plot(CART1_2, fallen.leaves = TRUE, extra = 106) # tree plotting
C1_2 # Cuts from VendorID and Total Amount 
predict_CART1_2 <-predict(CART1_2, test2019_2, type = 'class')
confmatrixCART1_2 <- table(test2019_2$binary, predict_CART1_2)
confmatrixCART1_2
CART1_2_accuracy <- sum(diag(confmatrixCART1_2)) / sum(confmatrixCART1_2)
CART1_2_accuracy  # 0.7289726
CART1_2_precision <- confmatrixCART1_2[4]/sum(confmatrixCART1_2[4],confmatrixCART1_2[2])
CART1_2_precision # 0.8239468
CART1_2_sensitivity <- confmatrixCART1_2[4] / sum(confmatrixCART1_2[4], confmatrixCART1_2[3])
CART1_2_sensitivity # 0.6929239
CART1_2_fscore <- (2 * (CART1_2_sensitivity * CART1_2_precision))/(CART1_2_sensitivity + CART1_2_precision)
CART1_2_fscore # 0.7527766

# Remove VendorID and implement original 80/20 split

CARTNoVID <- rpart(binary ~ passenger_count + trip_distance + total_amount + 
                   numeric_trip_time + time_of_day + dow + totamt_x_tripdist, data = train2019, method = 'class') 
CNoVID <- rpart.plot(CARTNoVID, fallen.leaves = TRUE, extra = 106) # tree plotting
CNoVID # Only divides the model from VendorID 
predict_CARTNoVID <-predict(CARTNoVID, test2019, type = 'class')
confmatrixCARTNoVID <- table(test2019$binary, predict_CARTNoVID)
confmatrixCARTNoVID
CARTNoVID_accuracy <- sum(diag(confmatrixCARTNoVID)) / sum(confmatrixCARTNoVID)
CARTNoVID_accuracy  # 0.5245843
CARTNoVID_precision <- confmatrixCARTNoVID[4]/sum(confmatrixCARTNoVID[4],confmatrixCARTNoVID[2])
CARTNoVID_precision # 0.9706882
CARTNoVID_sensitivity <- confmatrixCARTNoVID[4] / sum(confmatrixCARTNoVID[4], confmatrixCARTNoVID[3])
CARTNoVID_sensitivity # 0.5122532
CARTNoVID_fscore <- (2 * (CARTNoVID_sensitivity * CARTNoVID_precision))/(CARTNoVID_sensitivity + CARTNoVID_precision)
CARTNoVID_fscore #  0.6706106

# Make VendorID the DV for 80/20 split data

CART1_VID <- rpart(VendorID ~ passenger_count + trip_distance + total_amount + 
                 numeric_trip_time + time_of_day + dow + totamt_x_tripdist, data = train2019, method = 'class') 
C1_VID <- rpart.plot(CART1_VID, fallen.leaves = TRUE, extra = 106) # tree plotting
C1_VID # Only divides the model from VendorID 
predict_C1_VID <-predict(CART1_VID, test2019, type = 'class')
confmatrixCART1_VID <- table(test2019$VendorID, predict_C1_VID )
confmatrixCART1_VID
CART1VID_accuracy <- sum(diag(confmatrixCART1_VID)) / sum(confmatrixCART1_VID)
CART1VID_accuracy # 0.737714
CART1VID_precision <- confmatrixCART1_VID[4]/sum(confmatrixCART1_VID[4],confmatrixCART1_VID[2])
CART1VID_precision # 0.9346327
CART1VID_sensitivity <- confmatrixCART1_VID[4] / sum(confmatrixCART1_VID[4], confmatrixCART1_VID[3])
CART1VID_sensitivity # 0.9991871
CART1VID_fscore <- (2 * (CART1VID_sensitivity * CART1VID_precision))/(CART1VID_sensitivity + CART1VID_precision)
CART1VID_fscore # 0.9658324

# What we see here is that a lot of the other variables are accounted for by the VendorID - accuracy is 73.7% which 
# means that VendorID is predicted by the other variables really well. Hence, if VendorID can be predicted by the other
# variables, this means that VendorID is in effect taking their role when it is included in the model to predict binary (high tip, low tip)
# If VendorID is heavily predicted by the other variables, one can then say that when it is included in the original model,
# it takes all of the effects and CARRIES these impacts into the model where it is an independent variable. Therefore, justifying the split. 

# Use the 2019 trained data, and apply it to the 2020 data to assess for the impacts of COVID
# CART1 <- rpart(binary ~ VendorID + passenger_count + trip_distance + total_amount + 
#  numeric_trip_time + time_of_day + dow + totamt_x_tripdist, data = train2019, method = 'class') 
# C1 <- rpart.plot(CART1, fallen.leaves = TRUE, extra = 106) # tree plotting
# C1 # Only divides the model from VendorID 

predict_CART2 <-predict(CART1, merged2020, type = 'class')
confmatrixCART2 <- table(merged2020$binary, predict_CART2)
confmatrixCART2
CART2_accuracy <- sum(diag(confmatrixCART2)) / sum(confmatrixCART2)
CART2_accuracy # 0.6756459
CART2_precision <- confmatrixCART2[4]/sum(confmatrixCART2[4],confmatrixCART2[2])
CART2_precision # 0.7390731
CART2_sensitivity <- confmatrixCART2[4] / sum(confmatrixCART2 [4], confmatrixCART2[3])
CART2_sensitivity # 0.6577974
CART2_fscore <- (2 * (CART2_sensitivity * CART2_precision))/(CART2_sensitivity + CART2_precision)
CART2_fscore # 0.6960708
# The 2019 CART model can predict on the 2020 model with 67.5% accuracy

# 2020 model on 2020 model to see the way in which their data splits
m2020 <- sort(sample(nrow(merged2020), nrow(merged2020)*0.8))
train2020 <- merged2020[m2020,]
test2020 <- merged2020[-m2020,]
prop.table(table(train2020$binary)) # Represent the column figures as percentages of the total 
prop.table(table(test2020$binary)) 
CART3 <- rpart(binary ~ VendorID + passenger_count + trip_distance + total_amount + 
                 numeric_trip_time + time_of_day + dow + totamt_x_tripdist, data = train2020, method = 'class') 
C3 <- rpart.plot(CART3, fallen.leaves = TRUE, extra = 106) # tree plotting
C3 # Only divides the model from VendorID - same for 2020 as for 2019! 

# Out of curiosity, check if the data would be split at VendorID if we changed the definition of "high tip" 
# to 85th percentile of the data
median(merged2019$tipdivtotal) # 0.1665904
quantile(merged2019$tipdivtotal,0.85) # 0.17
# Create a new binary variable that is 1 if the tip is greater than the 85th percentile of tipdivtotal
merged2019$binary2 <- ifelse(merged2019$tipdivtotal>0.17,1,0)
head(merged2019$binary2)
str(merged2019$binary2)
str(merged2019)
y2019 <- sort(sample(nrow(merged2019), nrow(merged2019)*0.7))
ytrain2019 <- merged2019[y2019,]
ytest2019 <- merged2019[-y2019,]
prop.table(table(ytrain2019$binary2)) # Represent the column figures as percentages of the total 
prop.table(table(ytest2019$binary2)) 
ytrain2019$time_of_day <- as.integer(ytrain2019$time_of_day)
ytrain2019$dow <- as.integer(ytrain2019$dow)
ytrain2019$VendorID <- as.integer(ytrain2019$VendorID)
ytest2019$time_of_day <- as.integer(ytest2019$time_of_day)
ytest2019$dow <- as.integer(ytest2019$dow)
ytest2019$VendorID <- as.integer(ytest2019$VendorID)
yCART1 <- rpart(binary2 ~ VendorID + passenger_count + trip_distance + total_amount + 
                  numeric_trip_time + time_of_day + dow + totamt_x_tripdist, data = ytrain2019, method = 'class') 
yC1 <- rpart.plot(yCART1,fallen.leaves = TRUE, extra = 106) # tree plotting
yC1 # Divides from total_amount
predict_yCART1 <-predict(yCART1, ytest2019, type = 'class')
confmatrix_yCART1 <- table(ytest2019$binary, predict_yCART1)
confmatrix_yCART1
yCART1_accuracy <- sum(diag(confmatrix_yCART1)) / sum(confmatrix_yCART1)
yCART1_accuracy  # 0.5288411
yCART1_precision <- confmatrix_yCART1[4]/sum(confmatrix_yCART1[4],confmatrix_yCART1[2])
yCART1_precision # 0.06712013
yCART1_sensitivity <- confmatrix_yCART1[4] / sum(confmatrix_yCART1[4], confmatrix_yCART1[3])
yCART1_sensitivity # 0.919695
yCART1_fscore <- (2 * (yCART1_sensitivity * yCART1_precision ))/(yCART1_sensitivity + yCART1_precision )
yCART1_fscore # 0.1251097
# Conclusion: when we ask the model to predict the binary variable based on the 85th percentile of the data, it does not split on
# VendorID anymore. Rather, it begins to split on total_amount. The cost of this is a signficantly accuracy score = 53%. 
# The bigger trade off is the fscore which is at a mere 12% indicating that the model that splits from VendorID and which predicts on the median
# might be a better choice! Therefore, the validation of the CART model that splits from VendorID has been completed. 
