# 1) Data Cleaning 

rm(list = ls(all.names = TRUE))
gc()
cat("\014")  

april_to_june_2019 <- read.csv("/Users/SakinaMNazarali/Downloads/2019_Yellow_Taxi_Trip_Data_April_to_June.csv")
april_to_june_2020 <- read.csv("/Users/SakinaMNazarali/Downloads/2020_Yellow_Taxi_Trip_Data_April_to_June.csv")
taxi_zones <- read.csv("/Users/SakinaMNazarali/Downloads/taxi+_zone_lookup.csv")
nrow(april_to_june_2019) # 21772272
nrow(april_to_june_2020) # 1110871 

set.seed(12345)
install.packages("dplyr")
library(dplyr)
require(data.table)
require(pastecs)

# The 2019 dataset has over 21 million records. Randomly select 2 million records.
atj_2019 <- sample_n(april_to_june_2019,2000000)

nrow(atj_2019) # 2000000
atj_2020 <- april_to_june_2020
str(atj_2019)
str(atj_2020)

# The dates are currently incharacters format + AM/PM. In order to be able to perform an analysis on
# them, they need to be changed to POSIXct + military timing
str(atj_2019$tpep_pickup_datetime) #  chr 
str(atj_2019$tpep_dropoff_datetime) #  chr 
str(atj_2020$tpep_pickup_datetime) #  chr 
str(atj_2020$tpep_dropoff_datetime) #  chr 
install.packages("lubridate")
install.packages("tidyverse")
library(lubridate)
atj_2019$tpep_pickup_datetime = parse_date_time(atj_2019$tpep_pickup_datetime, c('Ymd HM', 'mdy IMS p'))
str(atj_2019$tpep_pickup_datetime) # POSIXct
atj_2019$tpep_dropoff_datetime = parse_date_time(atj_2019$tpep_dropoff_datetime, c('Ymd HM', 'mdy IMS p'))
str(atj_2019$tpep_dropoff_datetime) # POSIXct
atj_2020$tpep_pickup_datetime = parse_date_time(atj_2020$tpep_pickup_datetime, c('Ymd HM', 'mdy IMS p'))
str(atj_2020$tpep_pickup_datetime) # POSIXct
atj_2020$tpep_dropoff_datetime = parse_date_time(atj_2020$tpep_dropoff_datetime, c('Ymd HM', 'mdy IMS p'))
str(atj_2020$tpep_dropoff_datetime) # POSIXct
# A time that was previously written as "06/18/2019 03:38:34 PM" is now written as "2019-06-18 15:38:34"

# Ensure that the data we have is within the timeframe we are interested in: 
nrow(atj_2019) # 2000000
atj_2019 <- atj_2019[atj_2019$tpep_pickup_datetime >= "2019-04-01 00:00:00" & atj_2019$tpep_pickup_datetime <= "2019-06-30 11:59:59",]
nrow(atj_2019) # 1998403
nrow(atj_2020) # 1110871
atj_2020 <- atj_2020[atj_2020$tpep_pickup_datetime >= "2020-04-01 00:00:00" & atj_2020$tpep_pickup_datetime <= "2020-06-30 11:59:59",]
nrow(atj_2020) # 1109997
# Therefore, data records that were beyond our timeframe have now been ommitted. Note the difference in number of
# rows before filtering for the times.


# Check if the randomization was fair (only performed on 2019 data, given that the dataset was huge) and that each month has an approximately 
# equal number of records, subset the data by month:
april_2019 = atj_2019[atj_2019$tpep_pickup_datetime >= "2019-04-01 00:00:00" & atj_2019$tpep_pickup_datetime <= "2019-04-30 11:59:59",]
may_2019 = atj_2019[atj_2019$tpep_pickup_datetime >= "2019-05-01 00:00:00" & atj_2019$tpep_pickup_datetime <= "2019-05-31 11:59:59",]
june_2019 = atj_2019[atj_2019$tpep_pickup_datetime >= "2019-06-01 00:00:00" & atj_2019$tpep_pickup_datetime <= "2019-06-30 11:59:59",]
nrow(april_2019) # 673954
nrow(may_2019) # 687271
nrow(june_2019) # 618196

# To perform a similar check on 2020 data (even though the dataset was not randomly sampled), subset the data by months:
april_2020 = atj_2020[atj_2020$tpep_pickup_datetime >= "2020-04-01 00:00:00" & atj_2020$tpep_pickup_datetime <= "2020-04-30 11:59:59",]
may_2020 = atj_2020[atj_2020$tpep_pickup_datetime >= "2020-05-01 00:00:00" & atj_2020$tpep_pickup_datetime <= "2020-05-31 11:59:59",]
june_2020 = atj_2020[atj_2020$tpep_pickup_datetime >= "2020-06-01 00:00:00" & atj_2020$tpep_pickup_datetime <= "2020-06-30 11:59:59",]
nrow(april_2020) # 235508
nrow(may_2020) # 345519
nrow(june_2020) # 522945

# Drop unnecessary columns
ncol(atj_2019) # 18
atj_2019 = select(atj_2019, -c(store_and_fwd_flag,improvement_surcharge,congestion_surcharge, mta_tax))
ncol(atj_2019) # 14
ncol(atj_2020) # 18 
atj_2020 = select(atj_2020, -c(store_and_fwd_flag,improvement_surcharge,congestion_surcharge, mta_tax))
ncol(atj_2020) # 14

# For individuals who made a cash payment, the tip fare is recorded as 0, even though they may have made a tip. 
# The total fare of the trip is present in the dataset, but this would mean that tip/total fare = 0 for such rides
# which may skew the data.

# The number of trips that were performed using cash payment in the 2019 and 2020 dataset: 
sum(atj_2019$payment_type=='2') # 544475
sum(atj_2019$payment_type=='2')/nrow(atj_2019) # 0.2724586 = 27.2% of all trips in 2019 were paid via cash
sum(atj_2020$payment_type=='2', na.rm = TRUE) # 353495
sum(atj_2020$payment_type=='2', na.rm = TRUE)/nrow(atj_2020) # 0.3184648 = 31.8% of all trips in 2020 were paid via cash

# Drop all rows that were performed using a cash payment. payment_type == 2. 
nrow(atj_2019) # 1998377
atj_2019 <- subset(atj_2019, payment_type!="2")
nrow(atj_2019) # 1453902
nrow(atj_2020) # 1109997
atj_2020 <- subset(atj_2020, payment_type!="2")
nrow(atj_2020) # 629492

# Merge the location file to associate pick up and drop off locations with names over numbers.
# Whilst this does not necessarily make a difference in the analysis, it will in the interpretation. 
merged2019 <- merge(atj_2019,taxi_zones,by.x = c("PULocationID"),by.y = c("LocationID"))
names(merged2019)[names(merged2019) == 'Zone'] <- 'PUZone'
names(merged2019)[names(merged2019) == 'Borough'] <- 'PUBorough'
names(merged2019)[names(merged2019) == 'service_zone'] <- 'PUservice_zone'
merged2019 <- merge(merged2019,taxi_zones,by.x = c("DOLocationID"),by.y = c("LocationID"))
names(merged2019)[names(merged2019) == 'Zone'] <- 'DOZone'
names(merged2019)[names(merged2019) == 'Borough'] <- 'DOBorough'
names(merged2019)[names(merged2019) == 'service_zone'] <- 'DOservice_zone'
colnames(merged2019)

# [1] "DOLocationID"          "PULocationID"          "VendorID"              "tpep_pickup_datetime" 
# [5] "tpep_dropoff_datetime" "passenger_count"       "trip_distance"         "RatecodeID"           
# [9] "payment_type"          "fare_amount"           "extra"                 "tip_amount"           
# [13] "tolls_amount"          "total_amount"          "PUBorough"             "PUZone"               
#[17] "PUservice_zone"        "DOBorough"             "DOZone"                "DOservice_zone"       
# [21] "tipdivtotal"           "trip_time"             "numeric_trip_time"     "binary" 

merged2020 <- merge(atj_2020,taxi_zones,by.x = c("PULocationID"),by.y = c("LocationID"))
names(merged2020)[names(merged2020) == 'Zone'] <- 'PUZone'
names(merged2020)[names(merged2020) == 'Borough'] <- 'PUBorough'
names(merged2020)[names(merged2020) == 'service_zone'] <- 'PUservice_zone'
merged2020 <- merge(merged2020,taxi_zones,by.x = c("DOLocationID"),by.y = c("LocationID"))
names(merged2020)[names(merged2020) == 'Zone'] <- 'DOZone'
names(merged2020)[names(merged2020) == 'Borough'] <- 'DOBorough'
names(merged2020)[names(merged2020) == 'service_zone'] <- 'DOservice_zone'
colnames(merged2020)

# Checking if any columns have NA Values
names(merged2019)[sapply(merged2019, anyNA)] # "PUZone" "DOZone"
names(merged2020)[sapply(merged2020, anyNA)] # "PUZone" "DOZone"
# As mentioned earlier, the zone information (that were merged later on) are not of use in the analysis,
# but will come handy in the interpretation. Therefore, these data points will not need to be replaced or omitted.
# NAs are associated with Pick up and Drop Off locations 264 and 265. 

library(ggplot2)
ggplot(merged2019, aes(x=tip_amount,y=total_amount)) + geom_point() + geom_smooth(method=lm, se=FALSE, col='blue', size=1) + ggtitle("Tip/Total 2019 Data Points")
ggplot(merged2020, aes(x=tip_amount,y=total_amount)) + geom_point() + geom_smooth(method=lm, se=FALSE, col='red', size=1) + ggtitle("Tip/Total 2020 Data Points")

# Add a column which calculates the tip per trip as a fraction of the total fare
merged2019$tipdivtotal <- merged2019$tip_amount/merged2019$total_amount
merged2020$tipdivtotal <- merged2020$tip_amount/merged2020$total_amount
head(merged2019)
head(merged2020)

plot(merged2019$tip_amount, merged2019$tipdivtotal)
plot(merged2020$tip_amount, merged2020$tipdivtotal)

# Get rid of rows in the dataframe that have a negative tipdivtotal (You cannot have a negative tip)
min(merged2019$tipdivtotal) # -1.0142
dim(merged2019) # 1453348 
merged2019 <- subset(merged2019,tipdivtotal >= 0)
min(merged2019$tipdivtotal) # 0 
dim(merged2019) # 1453114 

min(merged2020$tipdivtotal, na.rm=TRUE) # -49.44776
dim(merged2020) # 629492 
merged2020 <- subset(merged2020,tipdivtotal >= 0)
min(merged2019$tipdivtotal) # 0 
dim(merged2020) # 629128 

# Get rid of rows in the dataframe that have negative tip_amount
min(merged2019$tip_amount) # -88.88
dim(merged2019)
merged2019 <- subset(merged2019, tip_amount >= 0)
min(merged2019$tip_amount) # 0 
dim(merged2019)

min(merged2020$tip_amount) # -36.3
dim(merged2020)
merged2020 <- subset(merged2020, tip_amount >= 0)
min(merged2020$tip_amount) # 0 
dim(merged2020)

library(ggplot2)
ggplot(merged2019, aes(x=tip_amount,y=tipdivtotal)) + geom_point() + geom_smooth(method=lm, se=FALSE, col='blue', size=1) + ggtitle("Tip/Total 2019 Data Points")
ggplot(merged2020 , aes(x=tip_amount,y=tipdivtotal)) + geom_point() + geom_smooth(method=lm, se=FALSE, col='red', size=1) + ggtitle("Tip/Total 2020 Data Points")

# Look at the tip_amount table and observe anomalies
summary(merged2019$tip_amount)
# Min. 1st Qu.  Median    Mean    3rd Qu.    Max. 
# 0.000   1.700   2.360   3.087   3.460 333.330 
summary(merged2020$tip_amount)
# Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
# 0.000   1.460   2.160   2.583   3.060 442.180 

# Looking at the maximum tip amounts, these could be heavily skewing the data. 
# Eliminate all rows with tips more than $50 
merged2019 <- subset(merged2019, tip_amount <= 50)
merged2020 <- subset(merged2020, tip_amount <= 50)

library(ggplot2)
ggplot(merged2019, aes(x=tip_amount,y=tipdivtotal)) + geom_point() + geom_smooth(method=lm, se=FALSE, col='blue', size=1) + ggtitle("Tip/Total 2019 Data Points")
ggplot(merged2020 , aes(x=tip_amount,y=tipdivtotal)) + geom_point() + geom_smooth(method=lm, se=FALSE, col='red', size=1) + ggtitle("Tip/Total 2020 Data Points")

# Check if tipdivtotal and tip_amount are normally distributed 
hist(merged2019$tipdivtotal)
hist(merged2020$tipdivtotal)

# Add a column that calculates the trip time (DOtime - PUtime)
merged2019$trip_time <- merged2019$tpep_dropoff_datetime - merged2019$tpep_pickup_datetime
head(merged2019$trip_time)
merged2020$trip_time <- merged2020$tpep_dropoff_datetime - merged2020$tpep_pickup_datetime
head(merged2020$trip_time)
str(merged2019$trip_time)

# Create a new column that holds the trip_time data as a numeric value, instead of a difftime value
merged2019$numeric_trip_time <- as.numeric(merged2019$trip_time)
str(merged2019$trip_time) # difftime
str(merged2019$numeric_trip_time) # num 
merged2020$numeric_trip_time <- as.numeric(merged2020$trip_time)
str(merged2020$numeric_trip_time) # num

# Get rid of rows in the dataframe that have a negative trip_time (You cannot have a negative travel time)
dim(merged2019) # 1453114 
min(merged2019$numeric_trip_time) # -344247
merged2019 <- subset(merged2019, numeric_trip_time >= 0)
min(merged2019$numeric_trip_time) # 0 
dim(merged2019) # 1453113 

dim(merged2020) # 629128
min(merged2020$numeric_trip_time) # -31873874
merged2020 <- subset(merged2020, numeric_trip_time >= 0)
min(merged2019$numeric_trip_time) # 0 
dim(merged2020) # 629127 

# Add a binary column that classifies the tip/totalamount as a binary variable based on the median tip amount
merged2019$binary <- ifelse(merged2019$tipdivtotal>median(merged2019$tipdivtotal,na.rm = TRUE),1,0)
merged2020$binary <- ifelse(merged2020$tipdivtotal>median(merged2020$tipdivtotal, na.rm = TRUE),1,0)
median(merged2020$tipdivtotal) # 0.166348

# Add a dummy variable that classifies thepickup time by the time of the day where if the pickup time is 
# between 00:00:00 and 05:00:00 = 1 Early Morning
# between 05:00:01 and 09:00:00 = 2 Morning Rush Hour
# between 09:00:01 and 16:00:00 = 3 Working Hours 
# between 16:00:01 and 20:00:00 = 4 Evening Rush Hour
# between 20:00:01 and 23:59:59 = 5 Night 
merged2019$PUdate <- as.Date(merged2019$tpep_pickup_datetime)
class(merged2019$PUdate)
merged2019$PUtime <- format(merged2019$tpep_pickup_datetime,format = "%H:%M:%S")
merged2019$PUtime <- as.times(merged2019$PUtime)
class(merged2019$PUtime)
library(chron)
breaks <- c('00:00:00', '05:00:00', '09:00:00', '16:00:00','20:00:00', '23:59:59')
labels <- c(1,2,3,4,5)
h1 <- chron(times=merged2019$PUtime)
br <- chron(times=breaks)
merged2019$time_of_day <-  cut(h1, br, labels=labels)
str(merged2019$time_of_day) # Factor
merged2019$time_of_day <- as.integer(merged2019$time_of_day)
head(merged2019)

merged2020$PUdate <- as.Date(merged2020$tpep_pickup_datetime)
class(merged2020$PUdate)
merged2020$PUtime <- format(merged2020$tpep_pickup_datetime,format = "%H:%M:%S")
merged2020$PUtime <- as.times(merged2020$PUtime)
class(merged2020$PUtime)
library(chron)
breaks <- c('00:00:00', '05:00:00', '09:00:00', '16:00:00','20:00:00', '23:59:59')
labels <- c(1,2,3,4,5)
h2 <- chron(times=merged2020$PUtime)
br <- chron(times=breaks)
merged2020$time_of_day <-  cut(h2, br, labels=labels)
str(merged2020$time_of_day) # Factor
merged2020$time_of_day <- as.integer(merged2020$time_of_day)
head(merged2020)

# Classify the days of the week using categorical/dummy variables therefore, to assign every date with a weekday.
# Weekdays are classified between 1-7 as shown below:
# Sunday = 1
# Monday = 2
# Saturday = 7
merged2019$dow <- as.POSIXlt(merged2019$PUdate)$wday + 1
str(merged2019$dow)
head(merged2019)
merged2020$dow <- as.POSIXlt(merged2020$PUdate)$wday + 1
str(merged2020$dow)
head(merged2020)

# Final check 
names(merged2019) # check titles for each of the column
head(merged2019,5) # check the first five rows for each column
str(merged2019) # check for the data types of each column (integer, num)
class(merged2019) # check for the data class of the dataframe
summary(merged2019) # perform quick overview of data
dim(merged2019) # check for the number of columns by rows

names(merged2020)
head(merged2020,5)
str(merged2020)
class(merged2020)
summary(merged2020)
dim(merged2020)

# 2) Summary Analytics

# Summary statistics for the merged2019 dataset
summary(merged2019)

# Summary statistics for the merged2020 dataset
summary(merged2020)

# Dividing the datasets based on high and low tip amounts
lowtips_2019 <- subset(merged2019, binary==0)
hightips_2019 <- subset(merged2019, binary==1)

# Summary analysis for 2019 dataset, for tips lower than the median
summary(lowtips_2019)

# Summary analysis for 2019 dataset, for tips higher than the median
summary(hightips_2019)

# Dividing the datasets based on high and low tip amounts
lowtips_2020 <- subset(merged2020, binary==0)
hightips_2020 <- subset(merged2020, binary==1)

# Summary analysis for 2019 dataset, for tips lower than the median
summary(lowtips_2020)

# Summary analysis for 2019 dataset, for tips higher than the median
summary(hightips_2020)

# Correlation Matrix 2019
cor(merged2019[, unlist(lapply(merged2019, is.numeric))], use="complete.obs") 

# Correlation Matrix 2002
cor(merged2020[, unlist(lapply(merged2020, is.numeric))], use="complete.obs") 

# Correlation Matrix visualizations
install.packages("corrplot")
library(corrplot)
corrMatrix2019 <- cor(select_if(merged2019,is.numeric), use="complete.obs")
corrplot(corrMatrix2019, method="color",tl.col="black",tl.cex = 0.5, col=colorRampPalette(c("black","white","gold"))(100))
corrMatrix2020 <- cor(select_if(merged2020,is.numeric), use="complete.obs")
corrplot(corrMatrix2020,method="color",tl.col="black",tl.cex = 0.5, col=colorRampPalette(c("black","white","gold"))(100))

# Create an interaction variable between total_amount and trip_time
merged2019$totamt_x_triptm <- merged2019$total_amount * merged2019$trip_time 
merged2020$totamt_x_triptm <- merged2020$total_amount * merged2020$trip_time 

# Histograms for 2019 Data
par(mfrow=c(2,2))
hist(hightips_2019$trip_distance, main = "Trip Distance for High Tip 2019 Trips", col = "darkmagenta", freq = FALSE, xlim=c(0,50), xlab = "Trip Distance in Miles")
hist(lowtips_2019$trip_distance, main = "Trip Distance for Low Tip 2019 Trips", col = "aquamarine", freq = FALSE, xlab = "Trip Distance in Miles")
hist(hightips_2019$passenger_count, main = "Passenger Count for High Tip 2019 Trips", freq = FALSE, col = "burlywood4", xlab = "Passenger Counts")
hist(lowtips_2019$passenger_count, main = "Passenger Count for Low Tip 2019 Trips", freq = FALSE, col = "coral2", xlab = "Passenger Counts")
hist(hightips_2019$VendorID, main = "VendorID for High Tip 2019 Trips", freq = FALSE, col = "blueviolet", xlab = "VendorID")
hist(lowtips_2019$VendorID, main = "VendorID for Low Tip 2019 Trips", freq = FALSE, col = "lightblue1", xlab = "VendorID")
hist(hightips_2019$fare_amount, main = "Fare Amount for High Tip 2019 Trips", freq = FALSE, col =  "mediumorchid1", xlab =  "Fare Amount in $")
hist(lowtips_2019$fare_amount, main = "Fare Amount for Low Tip 2019 Trips", freq = FALSE, col =  "navajowhite", xlab =  "Fare Amount in $")
hist(hightips_2019$tipdivtotal, main = "Tip/Total for High 2019 Trips", freq = FALSE, col = "palevioletred1", xlab = "Fraction", xlim=c(0,0.4))
hist(lowtips_2019$tipdivtotal, main = "Tip/Total for Low 2019 Trips", freq = FALSE, col = "seagreen2", xlab = "Fraction", xlim=c(0,0.2))
hist(hightips_2019$numeric_trip_time, main ="Trip Time in Seconds for High 2019 Trips", freq = FALSE, col = "tan3", xlab = "Time in Seconds", xlim = c(0,10000))
hist(lowtips_2019$numeric_trip_time, main ="Trip Time in Seconds for Low 2019 Trips", freq = FALSE, col = "yellow", xlab = "Time in Seconds", xlim = c(0,10000))

# Histograms for 2020 Data
hist(hightips_2020$trip_distance, main = "Trip Distance for High Tip 2020 Trips", col = "darkmagenta", freq = FALSE, xlim=c(0,50), xlab = "Trip Distance in Miles")
hist(lowtips_2020$trip_distance, main = "Trip Distance for Low Tip 2020 Trips", col = "aquamarine", freq = FALSE, xlim=c(0,50), xlab = "Trip Distance in Miles")
hist(hightips_2020$passenger_count, main = "Passenger Count for High Tip 2020 Trips", freq = FALSE, col = "burlywood4", xlab = "Passenger Counts")
hist(lowtips_2020$passenger_count, main = "Passenger Count for Low Tip 2020 Trips", freq = FALSE, col = "coral2", xlab = "Passenger Counts")
hist(hightips_2020$VendorID, main = "VendorID for High Tip 2020 Trips", freq = FALSE, col = "blueviolet", xlab = "VendorID")
hist(lowtips_2020$VendorID, main = "VendorID for Low Tip 2020 Trips", freq = FALSE, col = "lightblue1", xlab = "VendorID")
hist(hightips_2020$fare_amount, main = "Fare Amount for High Tip 2020 Trips", freq = FALSE, col =  "mediumorchid1", xlab =  "Fare Amount in $")
hist(lowtips_2020$fare_amount, main = "Fare Amount for Low Tip 2020 Trips", freq = FALSE, col =  "navajowhite", xlab =  "Fare Amount in $")
hist(hightips_2020$tipdivtotal, main = "Tip/Total for High 2020 Trips", freq = FALSE, col = "palevioletred1", xlab = "Fraction", xlim=c(0,0.4))
hist(lowtips_2020$tipdivtotal, main = "Tip/Total for Low 2020 Trips", freq = FALSE, col = "seagreen2", xlab = "Fraction", xlim=c(0,0.4))
hist(hightips_2020$numeric_trip_time, main ="Trip Time in Seconds for High 2020 Trips", freq = FALSE, col = "tan3", xlab = "Time in Seconds", xlim = c(0,10000))
hist(lowtips_2020$numeric_trip_time, main ="Trip Time in Seconds for Low 2020 Trips", freq = FALSE, col = "yellow", xlab = "Time in Seconds", xlim = c(0,10000))

par(mfrow=c(1,1))
